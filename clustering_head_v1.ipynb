{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.spatial.distance import cdist\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make syntetic data\n",
    "\n",
    "# Parameters for data generation\n",
    "n_samples = 200  # Number of data points\n",
    "n_features = 768  # Dimensionality of each data point\n",
    "centers = 4       # Number of clusters\n",
    "cluster_std = 5.0 # Standard deviation to add noise to the clusters\n",
    "noise_level = 30 # Adjust this to control the amount of noise\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=n_samples, \n",
    "                  n_features=n_features, \n",
    "                  centers=centers, \n",
    "                  cluster_std=cluster_std, \n",
    "                  random_state=42)\n",
    "\n",
    "# Adding noise to the data\n",
    "noise = np.random.normal(0, noise_level, X.shape)\n",
    "X_noisy = X + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TreeClusteringClassifier:\n",
    "    def __init__(self, max_clusters=10, min_clusters=2):\n",
    "        self.max_clusters = max_clusters\n",
    "        self.min_clusters = min_clusters\n",
    "        self.kmeans = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.class_names = ['1', '2', '3', '4']\n",
    "        self.class_mapping = None\n",
    "\n",
    "    def find_optimal_clusters(self, embeddings):\n",
    "        silhouette_scores = []\n",
    "        for k in range(self.min_clusters, self.max_clusters + 1):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(embeddings)\n",
    "            silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "        \n",
    "        optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + self.min_clusters\n",
    "        return optimal_clusters\n",
    "\n",
    "    def train_and_save_model(self, embeddings, labels, model_path, n_splits=5):\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        embeddings_scaled = self.scaler.fit_transform(embeddings)\n",
    "        \n",
    "        best_ari = -1\n",
    "        best_model = None\n",
    "        best_n_clusters = 0\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(embeddings_scaled)):\n",
    "            print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "            \n",
    "            train_embeddings = embeddings_scaled[train_idx]\n",
    "            val_embeddings = embeddings_scaled[val_idx]\n",
    "            val_labels = labels[val_idx]\n",
    "            \n",
    "            n_clusters = self.find_optimal_clusters(train_embeddings)\n",
    "            print(f\"Optimal number of clusters: {n_clusters}\")\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            kmeans.fit(train_embeddings)\n",
    "            \n",
    "            val_predictions = kmeans.predict(val_embeddings)\n",
    "            ari = adjusted_rand_score(val_labels, val_predictions)\n",
    "            print(f\"Fold {fold + 1} Adjusted Rand Index: {ari:.4f}\")\n",
    "            \n",
    "            if ari > best_ari or (ari == best_ari and n_clusters < best_n_clusters):\n",
    "                best_ari = ari\n",
    "                best_model = kmeans\n",
    "                best_n_clusters = n_clusters\n",
    "        \n",
    "        print(f\"Best Adjusted Rand Index: {best_ari:.4f}\")\n",
    "        print(f\"Best number of clusters: {best_n_clusters}\")\n",
    "        self.kmeans = best_model\n",
    "        self.class_mapping = self.map_clusters_to_classes(embeddings, labels)\n",
    "        self.save_model(model_path)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        joblib.dump({'kmeans': self.kmeans, 'scaler': self.scaler, 'class_mapping': self.class_mapping}, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        if os.path.exists(path):\n",
    "            model_data = joblib.load(path)\n",
    "            self.kmeans = model_data['kmeans']\n",
    "            self.scaler = model_data['scaler']\n",
    "            self.class_mapping = model_data['class_mapping']\n",
    "            print(f\"Model loaded from {path}\")\n",
    "        else:\n",
    "            print(f\"No saved model found at {path}\")\n",
    "\n",
    "    def predict(self, embeddings):\n",
    "        if self.kmeans is None:\n",
    "            raise ValueError(\"Model not trained or loaded. Please train or load a model first.\")\n",
    "        embeddings_scaled = self.scaler.transform(embeddings)\n",
    "        cluster_labels = self.kmeans.predict(embeddings_scaled)\n",
    "        distances = cdist(embeddings_scaled, self.kmeans.cluster_centers_, 'euclidean')\n",
    "        probabilities = np.exp(-distances)\n",
    "        probabilities /= probabilities.sum(axis=1, keepdims=True)\n",
    "        return cluster_labels, np.max(probabilities, axis=1)\n",
    "\n",
    "    def predict_with_class_names(self, embeddings):\n",
    "        cluster_labels, probabilities = self.predict(embeddings)\n",
    "        if self.class_mapping is None:\n",
    "            raise ValueError(\"Class mapping not available. Please train or load a model first.\")\n",
    "        predictions = [self.class_mapping.get(label, 'unknown') for label in cluster_labels]\n",
    "        return list(zip(predictions, probabilities))\n",
    "\n",
    "    def map_clusters_to_classes(self, embeddings, labels):\n",
    "        if self.kmeans is None:\n",
    "            raise ValueError(\"Model not trained or loaded. Please train or load a model first.\")\n",
    "        \n",
    "        n_clusters = self.kmeans.n_clusters\n",
    "        class_labels = np.unique(labels)\n",
    "        \n",
    "        cluster_to_classes = {}\n",
    "        for cluster in range(n_clusters):\n",
    "            cluster_indices = np.where(self.kmeans.labels_ == cluster)[0]\n",
    "            cluster_class_labels = labels[cluster_indices]\n",
    "            most_common_class = np.bincount(cluster_class_labels).argmax()\n",
    "            cluster_to_classes[cluster] = self.class_names[most_common_class]\n",
    "        \n",
    "        return cluster_to_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Optimal number of clusters: 2\n",
      "Fold 1 Adjusted Rand Index: 0.0806\n",
      "Fold 2/5\n",
      "Optimal number of clusters: 2\n",
      "Fold 2 Adjusted Rand Index: 0.2310\n",
      "Fold 3/5\n",
      "Optimal number of clusters: 3\n",
      "Fold 3 Adjusted Rand Index: 0.6455\n",
      "Fold 4/5\n",
      "Optimal number of clusters: 2\n",
      "Fold 4 Adjusted Rand Index: 0.1878\n",
      "Fold 5/5\n",
      "Optimal number of clusters: 2\n",
      "Fold 5 Adjusted Rand Index: 0.1720\n",
      "Best Adjusted Rand Index: 0.6455\n",
      "Best number of clusters: 3\n",
      "Model saved to best_tree_clustering_model.joblib\n",
      "Model loaded from best_tree_clustering_model.joblib\n",
      "\n",
      "Predictions for new data:\n",
      "Sample 1: Predicted class: 1, Probability: 0.8746\n",
      "Sample 2: Predicted class: 2, Probability: 0.4935\n",
      "Sample 3: Predicted class: 2, Probability: 0.7895\n",
      "Sample 4: Predicted class: 1, Probability: 0.8885\n",
      "Sample 5: Predicted class: 4, Probability: 0.8105\n",
      "Sample 6: Predicted class: 4, Probability: 0.7502\n",
      "Sample 7: Predicted class: 2, Probability: 0.8004\n",
      "Sample 8: Predicted class: 2, Probability: 0.4569\n",
      "Sample 9: Predicted class: 1, Probability: 0.8786\n",
      "Sample 10: Predicted class: 4, Probability: 0.7984\n",
      "Sample 11: Predicted class: 2, Probability: 0.4728\n",
      "Sample 12: Predicted class: 4, Probability: 0.8043\n",
      "Sample 13: Predicted class: 4, Probability: 0.8491\n",
      "Sample 14: Predicted class: 2, Probability: 0.7660\n",
      "Sample 15: Predicted class: 2, Probability: 0.7700\n",
      "Sample 16: Predicted class: 2, Probability: 0.4927\n",
      "Sample 17: Predicted class: 2, Probability: 0.7733\n",
      "Sample 18: Predicted class: 1, Probability: 0.8533\n",
      "Sample 19: Predicted class: 4, Probability: 0.7710\n",
      "Sample 20: Predicted class: 2, Probability: 0.4567\n",
      "Sample 21: Predicted class: 4, Probability: 0.8460\n",
      "Sample 22: Predicted class: 1, Probability: 0.8802\n",
      "Sample 23: Predicted class: 2, Probability: 0.4252\n",
      "Sample 24: Predicted class: 4, Probability: 0.8160\n",
      "Sample 25: Predicted class: 2, Probability: 0.4818\n",
      "Sample 26: Predicted class: 2, Probability: 0.4983\n",
      "Sample 27: Predicted class: 2, Probability: 0.7739\n",
      "Sample 28: Predicted class: 1, Probability: 0.8414\n",
      "Sample 29: Predicted class: 1, Probability: 0.8867\n",
      "Sample 30: Predicted class: 2, Probability: 0.7197\n",
      "Sample 31: Predicted class: 2, Probability: 0.4685\n",
      "Sample 32: Predicted class: 2, Probability: 0.7521\n",
      "Sample 33: Predicted class: 2, Probability: 0.8266\n",
      "Sample 34: Predicted class: 1, Probability: 0.9029\n",
      "Sample 35: Predicted class: 4, Probability: 0.7662\n",
      "Sample 36: Predicted class: 4, Probability: 0.7850\n",
      "Sample 37: Predicted class: 1, Probability: 0.8341\n",
      "Sample 38: Predicted class: 2, Probability: 0.4706\n",
      "Sample 39: Predicted class: 1, Probability: 0.8898\n",
      "Sample 40: Predicted class: 4, Probability: 0.7983\n",
      "Sample 41: Predicted class: 2, Probability: 0.7431\n",
      "Sample 42: Predicted class: 2, Probability: 0.7810\n",
      "Sample 43: Predicted class: 1, Probability: 0.8840\n",
      "Sample 44: Predicted class: 2, Probability: 0.4216\n",
      "Sample 45: Predicted class: 2, Probability: 0.4909\n",
      "Sample 46: Predicted class: 4, Probability: 0.7552\n",
      "Sample 47: Predicted class: 2, Probability: 0.4991\n",
      "Sample 48: Predicted class: 1, Probability: 0.8884\n",
      "Sample 49: Predicted class: 1, Probability: 0.8728\n",
      "Sample 50: Predicted class: 1, Probability: 0.8739\n",
      "Sample 51: Predicted class: 1, Probability: 0.8732\n",
      "Sample 52: Predicted class: 2, Probability: 0.7835\n",
      "Sample 53: Predicted class: 1, Probability: 0.8887\n",
      "Sample 54: Predicted class: 1, Probability: 0.8642\n",
      "Sample 55: Predicted class: 2, Probability: 0.8019\n",
      "Sample 56: Predicted class: 2, Probability: 0.7751\n",
      "Sample 57: Predicted class: 2, Probability: 0.7886\n",
      "Sample 58: Predicted class: 4, Probability: 0.7960\n",
      "Sample 59: Predicted class: 2, Probability: 0.7981\n",
      "Sample 60: Predicted class: 2, Probability: 0.4891\n",
      "Sample 61: Predicted class: 2, Probability: 0.4327\n",
      "Sample 62: Predicted class: 4, Probability: 0.8247\n",
      "Sample 63: Predicted class: 2, Probability: 0.4765\n",
      "Sample 64: Predicted class: 1, Probability: 0.8719\n",
      "Sample 65: Predicted class: 2, Probability: 0.4513\n",
      "Sample 66: Predicted class: 1, Probability: 0.8764\n",
      "Sample 67: Predicted class: 2, Probability: 0.4870\n",
      "Sample 68: Predicted class: 2, Probability: 0.8089\n",
      "Sample 69: Predicted class: 1, Probability: 0.8807\n",
      "Sample 70: Predicted class: 2, Probability: 0.7715\n",
      "Sample 71: Predicted class: 1, Probability: 0.8907\n",
      "Sample 72: Predicted class: 2, Probability: 0.5165\n",
      "Sample 73: Predicted class: 2, Probability: 0.7112\n",
      "Sample 74: Predicted class: 2, Probability: 0.5050\n",
      "Sample 75: Predicted class: 1, Probability: 0.8974\n",
      "Sample 76: Predicted class: 1, Probability: 0.8971\n",
      "Sample 77: Predicted class: 4, Probability: 0.7709\n",
      "Sample 78: Predicted class: 2, Probability: 0.7929\n",
      "Sample 79: Predicted class: 4, Probability: 0.8278\n",
      "Sample 80: Predicted class: 2, Probability: 0.8247\n",
      "Sample 81: Predicted class: 2, Probability: 0.7774\n",
      "Sample 82: Predicted class: 2, Probability: 0.8108\n",
      "Sample 83: Predicted class: 2, Probability: 0.7626\n",
      "Sample 84: Predicted class: 1, Probability: 0.8617\n",
      "Sample 85: Predicted class: 2, Probability: 0.4416\n",
      "Sample 86: Predicted class: 2, Probability: 0.4014\n",
      "Sample 87: Predicted class: 2, Probability: 0.7687\n",
      "Sample 88: Predicted class: 2, Probability: 0.7911\n",
      "Sample 89: Predicted class: 4, Probability: 0.7933\n",
      "Sample 90: Predicted class: 1, Probability: 0.8648\n",
      "Sample 91: Predicted class: 1, Probability: 0.8435\n",
      "Sample 92: Predicted class: 1, Probability: 0.8977\n",
      "Sample 93: Predicted class: 1, Probability: 0.4383\n",
      "Sample 94: Predicted class: 1, Probability: 0.4134\n",
      "Sample 95: Predicted class: 2, Probability: 0.7744\n",
      "Sample 96: Predicted class: 4, Probability: 0.7740\n",
      "Sample 97: Predicted class: 1, Probability: 0.8713\n",
      "Sample 98: Predicted class: 2, Probability: 0.4824\n",
      "Sample 99: Predicted class: 2, Probability: 0.7933\n",
      "Sample 100: Predicted class: 4, Probability: 0.8291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \n",
    "    embeddings = X_noisy\n",
    "    labels = y\n",
    "\n",
    "    classifier = TreeClusteringClassifier(max_clusters=10, min_clusters=2)\n",
    "\n",
    "    classifier.train_and_save_model(embeddings, labels, 'best_tree_clustering_model.joblib')\n",
    "\n",
    "    classifier.load_model('best_tree_clustering_model.joblib')\n",
    "\n",
    "    indices = np.random.choice(len(X), size=100, replace=False)\n",
    "    new_embeddings = X[indices]\n",
    "    \n",
    "    predictions = classifier.predict_with_class_names(new_embeddings)\n",
    "\n",
    "    print(\"\\nPredictions for new data:\")\n",
    "    for i, (class_name, probability) in enumerate(predictions):\n",
    "        print(f\"Sample {i + 1}: Predicted class: {class_name}, Probability: {probability:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
